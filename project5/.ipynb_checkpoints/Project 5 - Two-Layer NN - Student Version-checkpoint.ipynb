{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# CSCI 363 - Artificial Intelligence (Fall 2025) - Project 5\n",
    "\n",
    "**Due Date: Friday, December 12 by 11:59 PM ET**\n",
    "\n",
    "## Description\n",
    "In this assignment you will develop a neural network with fully-connected layers to perform classification using a toy dataset, after which you will train and evaluate a neural network classifier for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n",
    "\n",
    "## Instructions\n",
    "1. In this assignment you will write code to construct machine learning models and write responses to questions concerning the performance of said models. Please complete all sections below, adding new *Code* or *Markdown* cells as appropriate to answer the questions.\n",
    "2. You will **only** utilize elementary functions in `Pytorch` to implement your neural network, e.g.,\n",
    "    * *Matrix/Tensor Operations* such as `sum`, `mm`, `exp`, `log`, and `transpose`; and \n",
    "    * `clamp` - for implementing ReLU.\n",
    "\n",
    "## Running Code\n",
    "Though not required, this project is configured to use hardware (GPU) acceleration when available. **If you wish to use hardware acceleration, please [review these instructions](https://idsl.gryak.org/cs363/Python_Setup.html) to configure an environment via Google Colab, Kaggle, or your local hardware.**\n",
    "\n",
    "Instructions for Google Colab are included in the \"Google Colab Setup\" section.\n",
    "\n",
    "## Expectations\n",
    "You will **work independently** on the assignment. Please make use of the *Python/Pytorch/Data Science Reference Materials* posted on Brightspace, or **come to office hours should you need further assistance**.\n",
    "\n",
    "## Submission Instructions\n",
    "You will submit to Brightspace by the due date listed above:\n",
    "   * Your completed Jupyter notebook and `two_layer_net.py` files.\n",
    "   * Your computation graph image (Part 1.1)\n",
    "   * Your saved best model `nn_best_model.pt` for the CIFAR-10 dataset (Part 2.2)\n",
    "\n",
    "## Grading Rubric\n",
    "|**Part**|1.1|1.2|1.3|1.4|1.5|2.1|2.2|**Total**|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**%**|20|10|20|10|10|20|10|100|\n",
    "\n",
    "<sub><sup>Last modified by J. Gryak, Fall 2025.</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYE9thuXn4zP"
   },
   "source": [
    "# Part 0: Assignment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code (All Environments)\n",
    "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Jupyter Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "O3EvIZ0uAOVN"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "RANDOM_STATE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device/Precision Selection\n",
    "[Pytorch](https://pytorch.org/) and other libraries ([Tensorflow](https://www.tensorflow.org/), [JAX](https://docs.jax.dev/en/latest/)) enable computations to be performed on GPUs. In order for your code to take advantage of this hardware the various matrices/tensors must either be created on those GPUs or copied to them. The libraries for this project have been designed so that you can specify the device to use globally and the various routines will take care of this technicality for you. **However, if you create any temporary tensors in your code, you must specify the device manually.**\n",
    "\n",
    "You can use the conditional statement below to select the appropriate hardware for the project. Alternatively, you can simply hardcode your choice in the `device` variable. **Note that MPS devices only support 32-bit (single) precision.** For **this** project, we will use **32-bit (single) precision**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "#use single precision for Project 5\n",
    "dtype=torch.float32\n",
    "#if you have a CUDA-enabled nVidia GPU on your system, or are using Google Colab\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "    #dtype=torch.float64\n",
    "#if you have a Mac with an M1 or greater processor and macOS 13 or greater\n",
    "elif torch.backends.mps.is_available():\n",
    "    device=\"mps\"\n",
    "    #dtype=torch.float32\n",
    "#otherwise use the system's CPU\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "    #dtype=torch.float64\n",
    "\n",
    "print(f\"Using {device} device with {dtype} precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Google Colab Setup (Skip for Kaggle and Local Enviroments)\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['two_layer_net.py', 'Project 5 - Two-Layer NN - Student Version.ipynb', 'p5_utils.py']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2025FA folder and put all the files under Project5 folder, then '2025FA/Project5'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2025FA/Project5'\n",
    "if('google.colab' in sys.modules):\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None \n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from two_layer_net.py!\n",
    "Hello from p5_utils.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `two_layer_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if('google.colab' in sys.modules):\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "    \n",
    "    import time, os\n",
    "    os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "    time.tzset()\n",
    "    \n",
    "    from two_layer_net import hello_two_layer_net\n",
    "    hello_two_layer_net()\n",
    "    \n",
    "    from p5_utils import hello_helper\n",
    "    hello_helper()\n",
    "    \n",
    "    two_layer_net_path = os.path.join(GOOGLE_DRIVE_PATH, 'two_layer_net.py')\n",
    "    two_layer_net_edit_time = time.ctime(os.path.getmtime(two_layer_net_path))\n",
    "    print('two_layer_net.py last edited on %s' % two_layer_net_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# Part 1: Implement a Two-Layer Neural Network Using a Toy Dataset\n",
    "You will implement a **two-layer neural network**  for *multiclass* classification with a **softmax output**. The network will be optimized using **cross-entropy loss** and **$\\ell_2$ regularization** on the weight matrices. The network will use a **ReLU** activation function after the first fully connected layer. \n",
    "\n",
    "In other words, the network has the following architecture:\n",
    "\n",
    "> **Input** -> **Fully Connected Layer** -> **ReLU** -> **Fully Connected Layer** -> **Softmax**\n",
    "\n",
    "The outputs of the second fully-connected layer are the *scores* for each class, which are turned into *probabilities* by the softmax function.\n",
    "\n",
    "You will implement this two-layer network in **stages** in the Python file `two_layer_net.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T-4Phbd9GvI"
   },
   "source": [
    "## The Toy Dataset\n",
    "The inputs to the network will be a batch of $N$ (`num_inputs`) $D$-dimensional vectors (`input_size`); the hidden layer will have $H$ hidden units (`hidden_size`), and the output will be classification scores for $C$ categories (`num_classes`). This means that the learnable weights and biases of the network will have the following shapes:\n",
    "\n",
    "*   $W_1$: First layer weights; has shape $(D, H)$\n",
    "*   $b_1$: First layer biases; has shape $(H,)$\n",
    "*   $W_2$: Second layer weights; has shape $(H, C)$\n",
    "*   $b_2$: Second layer biases; has shape $(C,)$\n",
    "\n",
    "The function `p5_utils.get_toy_data` will generate random weights for this small toy dataset to help with model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Create a Computation Graph\n",
    "As shown in *Lecture 21 (slide 7)*, create a **computation graph** that depicts the *variables*, *functions*, and their *input/output* relationship.\n",
    "\n",
    "You may draw your graph by hand and scan it or use software to generate it, e.g., Microsoft Visio, [Draw.io](https://app.diagrams.net/). In either case, upload the picture with your submission and insert it into a cell below using Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLdCF3B-AOVT"
   },
   "source": [
    "## 1.2 - Forward Pass: Compute Scores\n",
    "Implement the forward pass of the network, which uses the weights and biases to compute scores for all inputs, in the function `nn_forward_pass` located in the Python file `two_layer_net.py`.\n",
    "\n",
    "Use the code below to compute the scores and compare with the answer. The total (sum) absolute difference should be smaller than `1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "tZV9_3ZWAOVU",
    "outputId": "7504b688-c002-4676-c064-29adc38f88a4"
   },
   "outputs": [],
   "source": [
    "from p5_utils import get_toy_data\n",
    "from two_layer_net import nn_forward_pass\n",
    "\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "toy_X, toy_y, params = get_toy_data(dtype=dtype,device=device)\n",
    "\n",
    "# TODO: Implement the score computation part of nn_forward_pass\n",
    "scores, _ = nn_forward_pass(params, toy_X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.tensor([\n",
    "        [ 9.7003e-08, -1.1143e-07, -3.9961e-08],\n",
    "        [-7.4297e-08,  1.1502e-07,  1.5685e-07],\n",
    "        [-2.5860e-07,  2.2765e-07,  3.2453e-07],\n",
    "        [-4.7257e-07,  9.0935e-07,  4.0368e-07],\n",
    "        [-1.8395e-07,  7.9303e-08,  6.0360e-07]], dtype=torch.float32, device=scores.device)\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-5\n",
    "p12_scores_diff = (scores.to(torch.float32) - correct_scores).abs().sum().item()\n",
    "print('Difference between your scores and correct scores: %.2e' % p12_scores_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XNJ3ydEAOVW"
   },
   "source": [
    "## 1.3 - Forward Pass: Compute Loss\n",
    "Now implement the first part of `nn_forward_backward` that computes the data and regularization loss.\n",
    "\n",
    "For the data loss, use **cross-entropy loss**. For the regularization loss, use  **$\\ell_2$ regularization** on the weight matrices $W_1$ and $W_2$; no regularization loss is applied to the bias vectors  $b_1$ and $b_2$.\n",
    "\n",
    "The code below will compute the loss on the toy data, and compare your answer to that computed by the reference implementation. The difference between the correct and computed loss should be less than `1e-4`.\n",
    "\n",
    "### Please Read!\n",
    "* You may find the writeup on softmax and cross-entropy loss in Chapter 4 of [*Dive into Deep Learning*](https://d2l.ai/d2l-en.pdf) helpful in implementing these functions.\n",
    "* When you implement the regularization over the $W$s, **please DO NOT multiply the regularization term by 1/2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wgG6w2uKAOVX",
    "outputId": "e198ce0f-1f05-431e-e724-240aeaa09b5e"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import get_toy_data\n",
    "from two_layer_net import nn_forward_backward\n",
    "\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "toy_X, toy_y, params = get_toy_data(dtype=dtype,device=device)\n",
    "\n",
    "# YOUR_TURN: Implement the loss computation part of nn_forward_backward\n",
    "loss, _ = nn_forward_backward(params, toy_X, toy_y, reg=0.05)\n",
    "print('Your loss: ', loss.item())\n",
    "correct_loss = 1.0986121892929077\n",
    "print('Correct loss: ', correct_loss)\n",
    "p13_diff = (correct_loss - loss).item()\n",
    "\n",
    "# should be very small, we get < 1e-4\n",
    "print('Difference: %.4e' % p13_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vExP-7n3AOVa"
   },
   "source": [
    "## 1.4 - Backward Pass\n",
    "The backward pass for the entire network has been implemented for you by Prof. Gryak in the function `nn_forward_backward`. Take a look at the code to get a sense of how this works.\n",
    "\n",
    "Run the following to verify that the manual backward pass is working correctly. You should get errors less than `1e-1` for single precision, and errors ranging from `1e-2` to `1e-11` when using double precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qCEkprvoAOVb",
    "outputId": "3e02d110-e672-4e33-80b8-5d898cfb30ef"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import get_toy_data\n",
    "from two_layer_net import nn_forward_backward\n",
    "\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "\n",
    "reg = 0.05\n",
    "toy_X, toy_y, params = get_toy_data(dtype=torch.float64,device=device)\n",
    "\n",
    "# Perform the gradient computation part of nn_forward_backward.\n",
    "loss, grads = nn_forward_backward(params, toy_X, toy_y, reg=reg)\n",
    "\n",
    "for param_name, grad in grads.items():\n",
    "  param = params[param_name]\n",
    "  f = lambda w: nn_forward_backward(params, toy_X, toy_y, reg=reg)[0]\n",
    "  grad_numeric = p5_utils.compute_numeric_gradient(f, param)\n",
    "  p14_errors = p5_utils.rel_error(grad, grad_numeric)\n",
    "  print('%s max relative error: %e' % (param_name, p14_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjAUalCBAOVd"
   },
   "source": [
    "## 1.5 - Train the Network\n",
    "1. Train your network on the toy dataset using **stochastic gradient descent (SGD)**. Look at the function `nn_train` in `two_layer_net.py` and fill in the missing sections to implement the training procedure.\n",
    "\n",
    "   You will also have to implement `nn_predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains. \n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. Your final training loss should be less than $1.0$.\n",
    "\n",
    "2. Using the function `p5_utils.plot_stats` to plot the loss function and train / validation accuracies.\n",
    "3. What do you observe about the training process and the performance of the model from these plots? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Wgw06cLXAOVd",
    "outputId": "be163c99-6590-4354-eafa-93d623bed3a8"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import get_toy_data\n",
    "from two_layer_net import nn_forward_backward, nn_train, nn_predict\n",
    "\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "toy_X, toy_y, params = get_toy_data(dtype=dtype,device=device)\n",
    "    \n",
    "# TODO: Implement the nn_train function.\n",
    "# You may need to check nn_predict function (the \"pred_func\") as well.\n",
    "p15_stats = nn_train(params, nn_forward_backward, nn_predict, toy_X, toy_y, toy_X, toy_y,\n",
    "                 learning_rate=1e-1, reg=1e-6,\n",
    "                 num_iters=200, verbose=False)\n",
    "\n",
    "print('Final training loss: ', p15_stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(p15_stats['loss_history'], 'o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "EUS4aDp_HzG1",
    "outputId": "e0c63f1f-2077-499f-c9c4-8a83e09635ae"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.plot(p15_stats['train_acc_history'], 'o', label='train')\n",
    "plt.plot(p15_stats['val_acc_history'], 'o', label='val')\n",
    "plt.title('Classification Accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cPIajWNAOVg"
   },
   "source": [
    "# Part 2: Train a Neural Network Classifier for the CIFAR-10 Dataset\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to train a classifier on a real dataset - CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "lYo_XrU3AOVg",
    "outputId": "e0e8ca93-3570-45f4-96ec-d03d91b1148c"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "\n",
    "#This function will download and preprocess the CIFAR-10 dataset to your project directory.\n",
    "data_dict = p5_utils.preprocess_cifar10(dtype=dtype,device=device)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq-HkgRBAOVQ"
   },
   "source": [
    "## 2.0 - Explore Training NNs on the CIFAR-10 Dataset\n",
    "Using your working implementation of the `TwoLayerNet` class you will investigate how the various hyperparameters affect model training and the ultimate performance of NN models. This part of the assignment is exploratory and will help you develop an intuition for how to identify issues in NN training, which can be used to inform your hyperparameter tuning strategy in Part 2.1.\n",
    "\n",
    "The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CsYAv3uAOVi"
   },
   "source": [
    "### 2.0.1 - Train an Initial Network on CIFAR-10\n",
    "Using the `TwoLayerNet`, this will train an initial model via your SGD implementation from Part 1. In addition, we will adjust the learning rate with an **exponential learning rate schedule** as optimization proceed, i.e., after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hgg0QV9DAOVj",
    "outputId": "ac949f3a-edf9-4a54-c47a-4c348c6c89e2"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "input_size = 3 * 32 * 32\n",
    "hidden_size = 36\n",
    "num_classes = 10\n",
    "\n",
    "# fix random seed before we generate a set of parameters\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, dtype=data_dict['X_train'].dtype, device=data_dict['X_train'].device)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(data_dict['X_train'], data_dict['y_train'],\n",
    "                  data_dict['X_val'], data_dict['y_val'],\n",
    "                  num_iters=500, batch_size=1000,\n",
    "                  learning_rate=1e-2, learning_rate_decay=0.95,\n",
    "                  reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = net.predict(data_dict['X_val'])\n",
    "val_acc = 100.0 * (y_val_pred == data_dict['y_val']).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixxgq5RKAOVl"
   },
   "source": [
    "### 2.0.2 - Investigate Training Performance\n",
    "With the default parameters provided above, you should get a validation accuracy less than 10% on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "6sYXImDTAOVm",
    "outputId": "54a06634-bf14-4bac-e522-b96800b6ae4b"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "from p5_utils import plot_stats\n",
    "\n",
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "FnuRjtyKAOVo",
    "outputId": "baeeddfd-8f05-40a9-ddfb-bbbf8736852b"
   },
   "outputs": [],
   "source": [
    "from p5_utils import show_net_weights \n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlVbXxmPNzPY"
   },
   "source": [
    "#### Interpreting the Results\n",
    "Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "Additionally, the visualization of the weights above seems to be primarily white noise, though there are some linear patterns visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDNZ8ZAnN7hj"
   },
   "source": [
    "### 2.0.3 - Investigating Capacity\n",
    "The initial model has very similar performance on the training and validation sets. This suggests that the model is **underfitting** and that its performance might improve if we were to increase its **capacity**.\n",
    "\n",
    "One way we can increase the capacity of a neural network model is to increase the size of its hidden layer. Here we investigate the effect of increasing the size of the hidden layer. The performance (as measured by validation set accuracy) should increase as the size of the hidden layer increases; however it may show diminishing returns for larger layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "_C-ChHUlN68f",
    "outputId": "c5fd64d9-1416-473d-b57d-877fc13f7cd6"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hidden_sizes = [2, 8, 32, 128] \n",
    "lr = 0.1\n",
    "reg = 0.001\n",
    "\n",
    "stat_dict = {}\n",
    "for hs in hidden_sizes:\n",
    "  print('train with hidden size: {}'.format(hs))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  p5_utils.reset_seed(RANDOM_STATE)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[hs] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpSrK3olUfOZ"
   },
   "source": [
    "### 2.0.3 - Investigating Regularization\n",
    "Another possible explanation for the small gap in performance between the train and validation accuracies of our model is **regularization**. In particular, if the regularization coefficient was too high then the model may be unable to fit the training data.\n",
    "\n",
    "We can investigate the phenomenon empirically by training a set of models with varying regularization strengths while fixing other hyperparameters.\n",
    "\n",
    "You should see that setting the regularization strength too high will harm the validation set performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "DRPsxxFnU3Un",
    "outputId": "2c631a91-a69a-4b62-e36b-29c829536b37"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lr = 1.0\n",
    "regs = [0, 1e-5, 1e-3, 1e-1]\n",
    "\n",
    "stat_dict = {}\n",
    "for reg in regs:\n",
    "  print('train with regularization: {}'.format(reg))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  p5_utils.reset_seed(RANDOM_STATE)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[reg] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zFWkxebWXtu"
   },
   "source": [
    "### 2.0.4 - Investigating Learning Rate\n",
    "Last but not least, we also want to see the effect of **learning rate** with respect to the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "lc_YYCDmWld-",
    "outputId": "19367e7d-0c8e-47fb-e895-a6e6b38934e6"
   },
   "outputs": [],
   "source": [
    "import p5_utils\n",
    "from p5_utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lrs = [1e-4, 1e-2, 1e0, 1e2]\n",
    "reg = 1e-4\n",
    "\n",
    "stat_dict = {}\n",
    "for lr in lrs:\n",
    "  print('train with learning rate: {}'.format(lr))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  p5_utils.reset_seed(RANDOM_STATE)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[lr] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVCEro4FAOVq"
   },
   "source": [
    "## 2.1 - Hyperparameter Tuning\n",
    "\n",
    "1. You will now attempt to train a better NN on the CIFAR-10 dataset via hyperparameter tuning. Implement the functions `find_best_net` and `nn_get_search_params` in `two_layer_net.py`. Then run the code in the next few blocks to\n",
    "   * return the best model and save it to disk,\n",
    "   * plot loss and learning rates, and\n",
    "   * compute validation set accuracy.\n",
    "To get full credit for the assignment, you should achieve a classification accuracy above $50\\%$ on the validation set.\n",
    "\n",
    "2. Visualize the weights of your best-performing network using `p5_utils.show_net_weights`. What visual patterns, if any, do you see in these weights, and how do they compare to the initial network you created in Part 2.0.1?\n",
    "\n",
    "### Please Read!\n",
    "**Tuning:** Tuning hyperparameters and developing an intuition for how they affect the final performance is a large part of using neural networks. Below, you should experiment with different combinations of the various **hyperparameters**, including *hidden layer size*, *learning rate*, *number of training epochs*, and *regularization strength*. You might also consider tuning the l*earning rate decay*, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Plots:** To guide your hyperparameter search, you might consider making auxiliary plots of training and validation performance as above, or plotting the results arising from different hyperparameter combinations. You should feel free to plot any auxiliary results you need in order to find a good network, but we won't require any particular plots from you.\n",
    "\n",
    "**Approximate Results:** Results will vary due to the stochastic nature of training and your chosen hyperparameters. Our best model gets a validation set accuracy 56.44% -- did you beat us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "colab_type": "code",
    "id": "bG4DjBMIAOVq",
    "outputId": "717919b5-b6da-44dd-c5c9-0b3f03b6b94b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import p5_utils\n",
    "from two_layer_net import TwoLayerNet, find_best_net, nn_get_search_params\n",
    "\n",
    "# running this model on float64 may needs more time, so set it as float32\n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "data_dict = p5_utils.preprocess_cifar10(dtype=torch.float32)\n",
    "\n",
    "# store the best model into this \n",
    "p5_utils.reset_seed(RANDOM_STATE)\n",
    "best_net, best_stat, best_val_acc = find_best_net(data_dict, nn_get_search_params)\n",
    "print('Best validation accuracy: %.2f%%' % best_val_acc)\n",
    "\n",
    "p5_utils.plot_stats(best_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NsYIu49plJ9r",
    "outputId": "a474ddcf-51fd-4e27-fced-38fd6ab0e37d"
   },
   "outputs": [],
   "source": [
    "# Check the validation-set accuracy of your best model\n",
    "y_val_preds = best_net.predict(data_dict['X_val'])\n",
    "p21_val_acc = 100 * (y_val_preds == data_dict['y_val']).double().mean().item()\n",
    "print('Best val-set accuracy: %.2f%%' % p21_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "hZgDq4zlAOVt",
    "outputId": "ba3ff655-7c19-4a14-d400-f5bc6094f309"
   },
   "outputs": [],
   "source": [
    "from p5_utils import show_net_weights\n",
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save your Model:** If you're happy with the model's performance, **run the following cell to save it**. \n",
    "\n",
    "We will also reload the model and run it on the training data to verify it has the correct weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "try:\n",
    "    path = os.path.join(GOOGLE_DRIVE_PATH, 'nn_best_model.pt')\n",
    "except NameError:\n",
    "    path = 'nn_best_model.pt'\n",
    "best_net.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UG56gKWsAOVv"
   },
   "source": [
    "## 2.2 - Evaluate Model on the Test Set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set. To get full credit for the assignment, you should achieve over $50\\%$ classification accuracy on the test set.\n",
    "\n",
    "**Note**: Do not run this until you are satisfied with your model's performance in Part 2.1 - otherwise this will result in data leakage. You must remove the text `**REMOVE AFTER COMPLETING 2.1**` for the cell below to run.\n",
    "\n",
    "(Our best model gets 56.03% test-set accuracy -- did you beat us?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2b3h8f8_AOVw",
    "outputId": "35cf6243-78a3-4c5a-b22c-5c9eae0d309d"
   },
   "outputs": [],
   "source": [
    "**REMOVE AFTER COMPLETING 2.1**\n",
    "y_test_preds = best_net.predict(data_dict['X_test'])\n",
    "p22_test_acc = 100 * (y_test_preds == data_dict['y_test']).double().mean().item()\n",
    "print('Test accuracy: %.2f%%' % p22_test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
